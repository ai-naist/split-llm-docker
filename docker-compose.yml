services:
  # --- Server Node (NVIDIA RTX 2060) ---
  server:
    build:
      context: .
      dockerfile: Dockerfile.server  # NVIDIA用ファイルを使用
    container_name: llm-server
    command: python -u server.py
    volumes:
      - .:/app
      - ./models:/root/.cache/huggingface
    ports:
      - "65432:65432"
    # ▼ ここが重要: GPUを割り当てる設定 ▼
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # --- Client Node (CPU / AMD iGPUはDockerでは難易度高) ---
  client:
    build:
      context: .
      dockerfile: Dockerfile.client  # CPU用ファイルを使用
    container_name: llm-client
    command: tail -f /dev/null
    volumes:
      - .:/app
      - ./models:/root/.cache/huggingface
    depends_on:
      - server
    environment:
      - SERVER_HOST=server

networks:
  default:
    driver: bridge